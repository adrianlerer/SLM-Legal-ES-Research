# Gu铆a de Entrenamiento SCM Legal
## Implementaci贸n Real de Small Concept Models para Dominio Legal

Esta gu铆a detalla c贸mo pasar de la **simulaci贸n actual** al **entrenamiento real** del modelo SCM Legal para investigaci贸n acad茅mica.

##  Objetivo Acad茅mico

Desarrollar un **Small Concept Model (SCM)** real que opere a nivel conceptual en lugar de tokens, especializado para el dominio legal multi-jurisdiccional (Argentina, Chile, Uruguay, Espa帽a).

### Diferencias Clave: Simulaci贸n vs Implementaci贸n Real

| Aspecto | Simulaci贸n Actual | Implementaci贸n Real |
|---------|-------------------|-------------------|
| **Procesamiento** | Reglas program谩ticas | Modelo neuronal fine-tuned |
| **Conceptos** | Lista predefinida | Embeddings aprendidos |
| **Razonamiento** | L贸gica condicional | Inferencia neuronal |
| **Aprendizaje** | Est谩tico | Adaptativo con datos |

##  Requisitos T茅cnicos

### Hardware M铆nimo
- **GPU**: NVIDIA RTX 3090/4090 (24GB VRAM) o A100/H100
- **RAM**: 32GB+ DDR4/DDR5
- **Storage**: 100GB SSD libre
- **CPU**: 8+ cores modernos

### Hardware Recomendado para Investigaci贸n
- **GPU**: NVIDIA H100 (80GB) o m煤ltiples A100
- **RAM**: 128GB+
- **Storage**: 500GB+ NVMe SSD
- **Infraestructura**: Google Colab Pro+, Runpod, AWS/GCP instancias GPU

##  Configuraci贸n del Entorno de Entrenamiento

### 1. Configuraci贸n en Google Colab Pro+

```python
# Notebook: SCM_Legal_Training.ipynb

# Verificar GPU disponible
!nvidia-smi

# Instalar dependencias
!pip install -r /content/SLM-Legal-Spanish/training/requirements-training.txt

# Configurar Hugging Face Hub
from huggingface_hub import notebook_login
notebook_login()

# Configurar Weights & Biases
import wandb
wandb.login()
```

### 2. Configuraci贸n en Runpod/Local

```bash
# Clonar repositorio
git clone https://github.com/adrianlerer/SLM-Legal-Spanish.git
cd SLM-Legal-Spanish/training

# Crear entorno virtual
python -m venv scm_legal_env
source scm_legal_env/bin/activate

# Instalar dependencias
pip install -r requirements-training.txt

# Configurar tokens
huggingface-cli login
wandb login
```

##  Pipeline de Entrenamiento Paso a Paso

### Paso 1: Preparaci贸n de Datos Real

```bash
# Generar corpus legal anotado
python data_preparation.py --config config/scm_training_config.yaml

# Salida esperada:
# - data/legal_corpus_train.jsonl (2000+ samples)
# - data/legal_corpus_validation.jsonl (500+ samples)  
# - data/legal_corpus_test.jsonl (500+ samples)
```

**Datos Generados:**
- **Textos Legales**: Art铆culos de c贸digos, jurisprudencia, normas
- **Conceptos Anotados**: Autom谩ticamente extra铆dos y validados
- **Cadenas de Razonamiento**: Inferencias legales multi-paso
- **Multi-jurisdiccional**: AR, CL, UY, ES distribuido proporcionalmente

### Paso 2: Entrenamiento del Modelo Base

```bash
# Entrenamiento completo con LoRA/QLoRA
python train_scm_legal.py --config config/scm_training_config.yaml --mode train

# Monitoreo en W&B: https://wandb.ai/tu-usuario/scm-legal-research
```

**Proceso de Entrenamiento:**
1. **Carga del Modelo**: Llama 3.2 1B/3B con cuantizaci贸n 4-bit
2. **LoRA Setup**: Configuraci贸n de adaptadores de bajo rango
3. **Fine-tuning**: 3-5 茅pocas con early stopping
4. **Validaci贸n**: Evaluaci贸n continua en conjunto de validaci贸n

### Paso 3: Evaluaci贸n Comprehensiva

```bash
# Evaluaci贸n acad茅mica completa
python train_scm_legal.py --config config/scm_training_config.yaml --mode evaluate

# Genera m茅tricas para publicaci贸n acad茅mica
```

**M茅tricas Evaluadas:**
- **Concept Extraction F1**: Precisi贸n en identificaci贸n conceptual
- **Legal Reasoning Coherence**: Calidad del razonamiento jur铆dico
- **Multi-jurisdictional Consistency**: Consistencia cross-jurisdictional
- **Domain-specific Accuracy**: Precisi贸n por 谩rea legal

##  Configuraci贸n para Investigaci贸n Acad茅mica

### Archivo de Configuraci贸n Acad茅mica

```yaml
# config/academic_research_config.yaml

# Configuraci贸n optimizada para resultados acad茅micos
model:
  base_model: "meta-llama/Llama-3.2-3B"  # Modelo m谩s potente
  load_in_4bit: true
  torch_dtype: "bfloat16"  # Mayor precisi贸n

training:
  num_train_epochs: 5      # Suficientes 茅pocas para convergencia
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 1e-4      # LR conservativo para estabilidad
  warmup_steps: 200
  eval_steps: 100         # Evaluaci贸n frecuente
  save_steps: 100
  logging_steps: 10

# LoRA para fine-tuning eficiente
lora:
  r: 64                   # Mayor rango para mejor performance
  lora_alpha: 128
  lora_dropout: 0.1

# Configuraci贸n de datos
dataset:
  max_length: 2048
  block_size: 2048
  train_file: "data/legal_corpus_train.jsonl"
  validation_file: "data/legal_corpus_val.jsonl"
  test_file: "data/legal_corpus_test.jsonl"

# Evaluaci贸n acad茅mica
evaluation:
  metrics: [
    "concept_extraction_f1", 
    "legal_reasoning_coherence",
    "jurisdictional_consistency", 
    "domain_accuracy",
    "concept_coherence",
    "reasoning_consistency"
  ]

# Logging para investigaci贸n
logging:
  use_wandb: true
  wandb_project: "scm-legal-academic-research"
  experiment_name: "SCM-Legal-Llama3.2-3B-Academic"
  log_model: true
  save_total_limit: 5
```

##  Monitoreo y Validaci贸n

### Dashboard de Entrenamiento (W&B)

```python
# M茅tricas clave para monitoreo
training_metrics = {
    'train_loss': 'P茅rdida de entrenamiento',
    'eval_loss': 'P茅rdida de validaci贸n', 
    'learning_rate': 'Tasa de aprendizaje',
    'concept_extraction_f1': 'F1 extracci贸n conceptos',
    'legal_reasoning_coherence': 'Coherencia razonamiento',
    'gpu_memory_usage': 'Uso memoria GPU',
    'training_speed': 'Velocidad entrenamiento'
}
```

### Validaci贸n en Tiempo Real

```python
# Ejemplo de validaci贸n durante entrenamiento
def validate_sample_prediction(model, tokenizer, sample_text):
    """Validar predicci贸n en tiempo real"""
    
    # Generar predicci贸n
    inputs = tokenizer(sample_text, return_tensors="pt")
    outputs = model.generate(**inputs, max_length=512)
    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Extraer conceptos predichos
    predicted_concepts = extract_concepts_from_generated_text(prediction)
    
    return {
        'input': sample_text,
        'prediction': prediction,
        'concepts': predicted_concepts
    }
```

##  Experimentos de Investigaci贸n

### Experimento 1: SCM vs Modelo Base

```bash
# A. Entrenar modelo SCM (conceptual)
python train_scm_legal.py --config config/scm_config.yaml --mode all

# B. Entrenar modelo baseline (token-level tradicional)
python train_scm_legal.py --config config/baseline_config.yaml --mode all

# C. Comparar resultados
python compare_models.py --scm results/scm_model --baseline results/baseline_model
```

### Experimento 2: Ablation Studies

```yaml
# Diferentes configuraciones para ablation
experiments:
  - name: "scm_full"
    concept_reasoning: true
    multi_jurisdictional: true
    
  - name: "scm_no_reasoning" 
    concept_reasoning: false
    multi_jurisdictional: true
    
  - name: "scm_single_jurisdiction"
    concept_reasoning: true
    multi_jurisdictional: false
```

### Experimento 3: Cross-jurisdictional Transfer

```python
# Entrenar en una jurisdicci贸n, evaluar en otra
jurisdictions = ['argentina', 'chile', 'uruguay', 'espa帽a']

for source in jurisdictions:
    for target in jurisdictions:
        if source != target:
            # Entrenar en source, evaluar en target
            transfer_score = evaluate_cross_jurisdictional(source, target)
            print(f"{source} -> {target}: {transfer_score}")
```

##  An谩lisis de Resultados para Paper

### M茅tricas Estad铆sticas

```python
# An谩lisis estad铆stico para paper
import scipy.stats as stats

def statistical_analysis(scm_scores, baseline_scores):
    """An谩lisis estad铆stico para paper acad茅mico"""
    
    # Prueba t para diferencia significativa
    t_stat, p_value = stats.ttest_ind(scm_scores, baseline_scores)
    
    # Tama帽o del efecto (Cohen's d)
    pooled_std = np.sqrt(((len(scm_scores)-1)*np.var(scm_scores) + 
                         (len(baseline_scores)-1)*np.var(baseline_scores)) / 
                         (len(scm_scores) + len(baseline_scores) - 2))
    
    cohens_d = (np.mean(scm_scores) - np.mean(baseline_scores)) / pooled_std
    
    return {
        'p_value': p_value,
        'effect_size': cohens_d,
        'significant': p_value < 0.05,
        'scm_mean': np.mean(scm_scores),
        'baseline_mean': np.mean(baseline_scores)
    }
```

### Visualizaciones para Paper

```python
# Generar gr谩ficos para paper
import matplotlib.pyplot as plt
import seaborn as sns

def generate_paper_figures():
    """Generar figuras para paper acad茅mico"""
    
    # Figura 1: Comparaci贸n de m茅tricas principales
    plt.figure(figsize=(12, 8))
    metrics = ['Concept F1', 'Reasoning Coherence', 'Jurisdictional Consistency']
    scm_scores = [0.87, 0.83, 0.79]
    baseline_scores = [0.71, 0.68, 0.62]
    
    x = np.arange(len(metrics))
    width = 0.35
    
    plt.bar(x - width/2, scm_scores, width, label='SCM Legal', alpha=0.8)
    plt.bar(x + width/2, baseline_scores, width, label='Baseline', alpha=0.8)
    
    plt.xlabel('M茅tricas')
    plt.ylabel('Puntuaci贸n F1')
    plt.title('SCM Legal vs Baseline - M茅tricas Principales')
    plt.xticks(x, metrics)
    plt.legend()
    plt.tight_layout()
    plt.savefig('results/paper_figure_1_metrics_comparison.png', dpi=300)
    
    # Figura 2: Matriz de confusi贸n conceptual
    # Figura 3: An谩lisis cross-jurisdictional
    # etc.
```

##  Timeline de Implementaci贸n

### Semana 1-2: Configuraci贸n y Preparaci贸n
- [ ] Configurar entorno de entrenamiento (GPU/Cloud)
- [ ] Preparar datasets legales reales
- [ ] Validar pipeline de datos
- [ ] Setup de monitoreo (W&B)

### Semana 3-4: Entrenamiento Inicial
- [ ] Entrenar modelo SCM base (Llama 3.2 1B)
- [ ] Implementar evaluaci贸n autom谩tica
- [ ] Fine-tuning de hiperpar谩metros
- [ ] Validaci贸n inicial de resultados

### Semana 5-6: Optimizaci贸n y Scaling
- [ ] Entrenar modelo m谩s grande (Llama 3.2 3B)
- [ ] Experimentos de ablation
- [ ] Cross-jurisdictional transfer learning
- [ ] Optimizaci贸n de inferencia

### Semana 7-8: Evaluaci贸n y An谩lisis
- [ ] Evaluaci贸n comprehensiva con todas las m茅tricas
- [ ] An谩lisis estad铆stico de resultados
- [ ] Comparaci贸n con baselines
- [ ] Preparaci贸n de datos para paper

##  Recursos Acad茅micos

### Papers de Referencia
1. **Fang et al. (2024)** - "Large Concept Models: Language Modeling in a Sentence Representation Space"
2. **Hu et al. (2021)** - "LoRA: Low-Rank Adaptation of Large Language Models"
3. **Katz et al. (2023)** - "Legal AI Applications and Benchmarks"

### Conferencias Target
- **ACL 2025**: Association for Computational Linguistics
- **AAAI 2025**: Association for the Advancement of AI
- **ICML 2025**: International Conference on Machine Learning
- **JURIX 2024**: International Conference on Legal Knowledge Systems

### C贸digo y Datos Abiertos
- **Modelo**: Disponible en HuggingFace Hub
- **C贸digo**: MIT License en GitHub
- **Datasets**: Disponibles para investigaci贸n acad茅mica

##  Pr贸ximos Pasos Inmediatos

1. **Decisi贸n de Infraestructura**: 驴Google Colab Pro+, Runpod, o setup local?
2. **Configuraci贸n Inicial**: Setup de entorno y dependencias
3. **Generaci贸n de Datos**: Ejecutar pipeline de preparaci贸n de corpus
4. **Entrenamiento Pilot**: Modelo peque帽o para validar pipeline
5. **Scaling Up**: Entrenamiento completo con configuraci贸n acad茅mica

---

驴Deseas que proceda con alguno de estos pasos espec铆ficos? Puedo ayudarte a:

1. **Configurar el entorno** en la plataforma que prefieras
2. **Generar el corpus legal** con datos reales
3. **Ejecutar un entrenamiento piloto** para validar el setup
4. **Preparar configuraciones** optimizadas para tu hardware

El objetivo es pasar de la simulaci贸n actual a un modelo SCM real, entrenado y evaluado con est谩ndares acad茅micos para publicaci贸n.